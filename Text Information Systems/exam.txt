Week 7:
	Overview Text Mining and Analytics: Part 1:
		Turning text data into high-quality information or actionable knowledge to minimize human effort and supply knowledge for optimal decision making
		Related to text retrieval, which is an essential component in any text mining system
		Text data is generated by using humans as subjective sensors that perceive the real world and express it as text
		Other things that monitor things in real life (like a thermometer) also generate non-text data (such as numerical, categorical, relational, video)
		Data mining problem is to turn all data (non-text and text) into actionable knowledge by using mining techniques for each of the different kinds of data. Our focus is specifically on text mining
	
	Overview Text Mining and Analytics: Part 2:
		Real world -> (perspective) -> Observed World -> (English) -> Text Data so the text data is affected by the persons perspective and the language used to express it
		Text mining is an attempt to revert the process (ie. going from text data -> English). This is mining knowledge about language
		Going from text data -> observed world is mining the content of the text data
		Going from text data -> the observer that observed the world is mining knowledge about the observer (such as their mood or sentiment)
		So you can mine both factual content about the world as well as opinions from the observer
		Going from text data -> real world is inferring other real-world variables (also called predictive analytics)
		Mining information of previous steps can help infer the real-world variables
		We can also use non-text data and the context associated with the text data (such as time, location, other metadata) to help with real-world variable predictions as well
		
	Natural Language Content Analysis: Part 1:
		Lexical analysis (or part of speech tagging) is identifying if words are nouns/verbs/etc.
		Syntactic analysis (or parsing) is finding structural relations within the sentence such as noun phrases/verb phrases/etc.
		Semantic analysis is getting the actual meaning of the sentence in a formal way
		We can also infer more knowledge ie. if someone is being chased, they are scared
		Pragmatic analysis (speech act) is inferring what the sentence is requesting or understanding why it was said
		NLP is difficult because natural language is designed to make human communication efficient so we omit common sense knowledge and keep a lot of ambiguities which makes it hard for a computer to figure out
		Word level amibguity is when words can have multiple meanings or be different parts of speech
		Syntactic amibguity is when the sentence can be understood in different ways like "a man saw a boy with a telescope"
		Anaphora resolution is when it isn't clear what is being referred to like the word himself in "John persuaded Bill to buy a TV for himself"
		Presupposition is when a phrase implies prior information like "he has quit smoking"
		Computers can't do any of theses tasks perfectly 
			We can do POS tagging well but not perfectly
			We can do partial parsing pretty well (not complete)
			We can do entity/relation extraction, word sense disambiguation, and sentiment analysis but not all semantic analysis
			Inferencing and speech act analysis is still difficult
		
	Natural Language Content Analysis: Part 2:
		Robust and general NLP tends to be shallow while deep understanding does not scale up
		NLP is the foundation for text mining
		Since deep NLP requires common sense knowledge and inferences, it only works for very limited domains but shallow NLP based on statistical methods can be done in large scale so it os more broadly applicable
		In practice, statistical NLP is the basis and humans provide help as needed
		
	Text Representation: Part 1:
		Text can be represented as a string of characters which is the most general way (but not very meaningful)
		It can also be represented as a sequence of words (ie. segmenting by spaces instead of characters)
		We can add POS tags to the sequence of words representation and we can also add syntactic structures, entities and relations and the more we add, the more useful the representation is and the less robust it gets
		We can go even further by adding logic predicates and speech acts 
		The more we add to the representation, the deeper the NLP gets, more human effort is required, it's less accurate when done by computers and we get closer to knowledge representation
		
	Text Representation: Part 2:
		Each level of text representation enables more analysis
			String - string processing
			Words - word relation, topic, sentiment analysis
			Syntactic structures - syntactic graph analysis
			Entites+relations - knowledge graph, information network analysis
			Logic predicates - integrative analysis of scattered knwoledge, logic inference
		Text representation determines what kind of mining algorithms can be applied
	
	Word Association Mining and Analysis:
		Two words have paridigmatic relations if they are in the same class and can be substituted for each other (like cat and dog)
		Two words have syntagmatic relations if they can be combined with each other (like cat and sit)
		These two relations are basic and complementary and can be generalized to describe relations of any items in a language (ie. using phrases instead of words)
		Mining word associations is useful for improving accuracy of many NLP tasks such as POS tagging, parsing, etc. and is useful for many applications like text retrieval
		Paradigmatically related words have similar context (the words in the sentence if you remove the related words)
		We can split the context into left context (words that appears before the related word) and right context (word that appears after the related word) and general context (rest of the sentence) 
		For syntagmatically related words, we can just look at the words that occur right before and right after the related words (ie. how helpful is the occurrence of word 1 for predicting occurrence of word 2)
		General idea:
			Represent each word by its context, compute context similarity, and words with high context similarity likely have paradigmatic relations
			Count how many times two words occur together in a context, compare their co-occurrences with their individual occurrences and words with high co-occurrences but relatively low individual occurrences likely have syntagmatic relation
		Paradigmatically related words tend to have syntagmatic relation with the same words
		
	Paradigmatic Relation Discovery Part 1:
		Treat the context of a word as a pseudo document (bag of words)
		We can take different kinds of context like left, right or a window of words around the word
		We want to calculate the similarity of two words, we look at the similarity of their context (we can sum up similarity of left, right and window contexts)
		Think of the bag of words as a vector space model, with a dimension for each word in the vocabulary
		We can use a frequency vector for each context, where we count the number of times that a word appears in the context. So each vector represents the context of a word
		Expected Overlap of Words in Context (EOWC) is when we the vectors are defined as the normalized count of the word in the context (c(w,d1)/|d1|) and the similarity is the dot product. This similarity gives the probability that two randomly picked words from d1 and d2 are identical
		Some issues with EOWC is that it favors matching one frequent term over matching more distinct terms and it treates every word equally (overlap on "eats" isn't as meaningful as overlap on "eats")
		
	Paradigmatic Relation Discovery Part 2:
		To address EOWC favoring one frequent term over more distinct ones, we can use a sublinear transformation of TF
		To address EOWC weighing all words equally, we introduce IDF
		For the TF transformation, we map c(w,d) to TF(w,d) where the TF function is something between a linear relation between count and TF and a 0/1 bit vector
		A commonly used function is the BM25 transformation (TF=((k+1)x)/c(w,d)+k) where k+1 is the upper bound of the function
		For IDF weighting (IDF(w)), we use IDF(w)=log[(M+1)/k] where M is the total number of docs, and k is the total number of docs containing w. This has an upper bound of log(M+1). In this case, our collection is the contexts
		So now we can represent contexts as a vector of normalized BM25 weights (BM25 weight for the word/BM25 weight for all words) and the dot product similarity function now multiplies each term by the words IDF too. We also add another parameter b to control the amount of length normalization
		BM25 can also be used for syntagmatic relations. Now we represent the context vectors as their BM25 weights, multiplied by the words IDF weighting. So words with a high weight in these context vectors likely have syntagmatic relation to the word the vector represents
		
		
		
Week 8:
	Syntagmatic Relation Discovery: Entropy:
		Intuitively, when predicting whether words are present in a segment, very rare or very common words are easier to guess 
		Formally define the problem as predicting the value of a binary random variable X (1 = word w is present, 0 = otherwise) so the more random X is, the more difficult the prediction would be
		We introduce entropy (H(X)) which measures the randomness of X. It is defined as -p(X=v)*log_2(p(X=v)) for all possible values of X (v). This function is at its peak when the probability that X=1 (and 0) is exactly 0.5
		In the example of flipping a coin, if the coin is biased (always lands on heads), the entropy is 0. For a regular coin, the entropy is 1 so high entropy words (more random) are harder to predict. 
		We would assume that entropy for "the" to be close to 0 since it occurs everywhere and the entropy of "meat" to be high
	
	
	Syntagmatic Relation Discovery: Conditional Entropy:
		If we know the presence/absence of a word, does it change the entropy of another word? 
		Before, we were only considering p(X_meat=1 or 0) but not we will look at p(X_meat=1 or 0 | X_eats=1)
		So now, H(X_meat|X_eats) = p(X_eats=u)*H(X_meat|X_eats=u) for all possible values of X (u)
		In general, for any discrete random variables X and Y, H(X) >= H(X|Y). So by knowing more info, we can only reduce the entropy
		The value of H(X_meat|X_meat) is 0 since there is no uncertainty anymore. This is also the lower bound
		H(X_meat|X_the) would likely be higher than H(X_meaet|X_eats) because knowing that "the" is there, shoudln't change the entropy much but knowing "eats" is there should
		H(X_meat|X_w) would reach its maximum (H(X_meat)) when w has nothing to do with "meat"
		To mine syntagmatic relations using conditional entropy, for each word 1:
			Compute conditional entropy for each other word W2 X(X_w1|X_w2)
			Sort all candidate words in ascending order of conditional entropy
			Take the top ranked words as the ones with potential syntagmatic relation (would need to use a threshold for each W1)
		H(X_w1|X_w2) is comparable with H(X_w1|X_w3) but H(X_w1|X_w2) and H(X_w3|X_w2) are not because of the different upper bounds
		
	Syntagmatic Relation Discovery: Mutual Information: Part 1:
		To compare conditional entropy across different words, we introduce mutual information (I(X;Y)) which is the amount of reduction in entropy for X we can obtain by knowing Y
		I(X;Y)=H(X)-H(X|Y)-H(Y)-H(Y|X)
		I(X;Y) is non-negative, symmetric, and =0 iff X and Y are independent
		This allows us to compare different X,Y pairs
		This can be used to find words that have high mutual information with a given word so I(X_eats;X_meat) > I(X_eats;X_the)
		Note that I(X_w;X_w)=H(X_w)
		I(X_w1;X_w2) can be rewritten using KL divergence where the numerator is the observed joint distribution of X_w1 and X_w2 divided by the expected joint distribution of them if X_w1 and X_w2 were independent. We sum these values over all possible combinations of values of the two variables
		We are measuring the divergence of the actual joint distribution from the expected joint distribution under the independence assumption. The larger the divergence, the higher the mutual information (MI) would be
		The KL divergence version of MI uses probabilities that sum up to 1, such as p(X_w=1) + p(X_w=0) = 1 which we can use to compute values if we know certain probabilities
		Specifically, we only need to know p(X_w1=1), p(X_w2=1) and p(X_w1=1,X_w2=1)
		
	Syntagmatic Relation Discovery: Mutual Information: Part 2:
		We can calculate p(X_w1=1), p(X_w2=1) and p(X_w1=1,X_w2=1) by taking the count of segments that each w1, w2, and both and dividing it by the number of segments to normalize
		In case we have 0 counts, we can add psuedo data by adding pseudo segments for each scenario (only one word in the segment, both words, or neither). We would treat all of these pseudo segments as one segment so they are all weighted a quarter of one segment
		
	Topic Mining and Analysis: Motivation and Task Definition:
		We can use text data to determine topics and use those topics to get knowledge about the world
		We can also add non-text data (context, location, time) to our text data to help find topics
		Tasks are discovering k topics, and then determining which topics a document covers
		More formally, we have a collection C of N documents and a number of topics k as input, and we output what the k topics are (theta), and the coverage of topics in each doc (pi_ij = prob of doc i covering topic j)
		
	Topic Mining and Analysis: Term as Topic:
		The initial idea is that a topic is defined as a single term
		To mine k topics from our collection C, we can parse text in C to obtain candidate terms. Then we design a scoring function to measure how good each term is as a topic
			Favor a representative term (high frequency)
			Avoid words that are too frequent (can use TF-IDF weighting)
			Domain specific heuristics are also possible (like hashtags on a tweet)
		Then we pick k terms with the highest scores but try to minimize redundancy (remove closely related terms)
		To compute doc coverage, the simplest approach would be to count and normalize word occurrences but there are obvious issues with this
			Lack of expressive power (can only represent simple topics)
			Incompletness of vocab coverage (can't capture related words)
			Word sense ambiguity (the topic a term is relate to can be ambiguous like "star")
			
	Topic Mining and Analysis: Probabilistic Topic Models:
		A probabilistic topic model will address all of the issues from above
		Instead, represent a topic as a word distribution where the high probability words are words related to the topic and vice versa
		More formally, we have a collection C of N documents, a set of vocabulary sets V, and a number of topics k as the input, and we output what the k topics are as a word distribution (theta), and the coverage of topics in each doc (pi_ij = prob of doc i covering topic j) 
		Generative model for text mining:
			Modeling of data generation: P(Data | Model, lambda) where lambda is all of the parameters (theta_i, pi_ij). There would be k thetas (each theta is a set of probability values), and N*k pis
			Parameter estimation/inferences: lambda* = argmax_lambda  p(Data|Model, lambda). We are finding the set of parameters that maximize the probability of the observed data
			Take lambda* as the "knowledge" to be mined for the text mining problem
			Adjust the design of the model to discover different knowledge
			
	Probabilistic Topic Models: Overview of Statistical Language Models: Part 1:
		Statistical Language Model (LM) is a probability distribution over word sequences and is context dependent
		It can alo be regarded as a probabilistic mechanism for generating text (also called a "generative" model)
		The simplest language model is the unigram language model where each word is generated independently (so probability for generating each word can be multiplied together for phrases)
		Assume text is a sample drawn according to this word distribution
		We can also use the text to estimate a language model assuming you know the topic of the text
		The maximum likelihood estimate would be to estimate that the probability of a word in a language model is equal to the amount of its occurrences/number of words in the document
			
	Probabilistic Topic Models: Overview of Statistical Language Models: Part 2:
		Maximum likelihood estimation is when the best result means that the data likelihood reaches maximum (theta* = argmax_theta P(X|theta)). Problem with this is when the data is too small (few data points) then we would be biased towards just the data available
		Bayesian estimation is when the best result means being consistent with our prior knowledge and explaining data well. (theta* = argmax_theta P(theta|X) = argmax_theta P(X|theta)P(theta) <- this is called the MAP estimate)
		Bayes rule is that p(X|Y) = p(Y|X)p(X)/p(Y)
		Problem with bayesian estimation is defining the prior
		We call the point where p(theta) is the highest the prior mode and the point where p(X|theta) is the highest the ML estimate. Then the posterior mode theta_1 is the point where MAP is the highest and is between the prior mode and ML estimate
		In Bayesian inference we're interested in the distribution of all of the parameters
		The problem of Bayesian inference is to infer the posterior distribution and other quantities depending on theta f(theta). We don't know what theta is yet so we can say that f*(theta) = f(theta)p(theta|X) for all thetas
		Posterior theta* is theta * p(theta|X) for all theta
			
	Probabilistic Topic Models: Mining One Topic:
		The simplest case is when you have one document and one topic. Then the input is the collection (single document) and the vocabulary (don't need k since we know it's 1) and the output is the single theta which is a probability distribution for the words
		In this case the setup is:
			Data is a single document represented as a sequence of words
			Model is a unigram LM theta
			Likelihood function is p(d|theta)=product of theta_i ^ c(w_i,d) for all words in the LM (theta_i is the likelihood of word i in the vocab)
			ML estimate is argmax_theta product of theta_i ^ c(w_i,d) for all words in the LM
		We can maximize the log likelihood for mathematical convenience (changes the product to a sum which is easier for taking the derivative)
		This gives us argmax_theta sum of c(w_i,d)*log(theta_i) for all words in the vocabulary
		We can use a Lagrange function which will combine our objective function with another function for the constraint that all theta_i need to add up to 1. This is to convert a constrained optimization problem to an unconstrained one but also introduces a new lambda variable
		If we take the partial derivative of the Lagrange function wrt theta_i and set it to 0, we can see the relation between theta_i and lambda. Then using the constraint on theta_i, we can solve for lambda and after doing so, we can conclude that the optimal theta_i is c(w_i,d)/|d|
		Say we had a text mining paper, then p(w|theta) would be proportionate to the number of times those words occur but we want to get get rid of the common words
		
		
		
Week 9:
	Probabilistic Topic Models: Mixture of Unigram Language Models:
		To factor out common words, we can add another language model (the background language model) and combine it with the original one
		So you have one distribution for your topic with high probabilities for the topic specific words, and one for the background with high probabilities for common words. You also need to define the probability of choosing each of these models (maybe 50% by default but can be changed)
		This is called a mixture model
		Now the probability of generating a word w is calculated as p(theta_d)*p(w|theta_d) + p(theta_B)*p(w|theta_B) so you have both models and the probability of selecting the model
		The purpose of the mixture model is to treat everything as a single generative model for generating words
		In this case, estimating the model "discovers" two topics and topic coverage
		The mixture model is a more general model because you can set p(theta_d) or p(theta_B) to 1 and the other one has to be 0
		In the mixture model, the set up is:
			Data is the document
			Mixture models parameters are p(theta_d), p(w|theta_d), p(theta_B), p(w|theta_B) and the thetas are unigram language models
			Likelihood function is the product of the probability of generating each word in the document to the power of the count of each word
			The ML estimate is the argmax for the parameters of the likelihood function
		
	Probabilistic Topic Models: Mixture Model Estimation: Part 1:
		To help filter out background words, assume we know all of the parameters for a mixture model except for the word distribution of the topic model
		The ML estimate will demote background words in the topic model because words that are more likely to occur in the background model will tend to be less likely to occur with the topic model in order to optimize the probability of generating any given word
			So if word 1 has high probability in the background model and word 2 has low probability, the maximum likelihood for a document containing just one occurrence of those two words would be achieved by having the topic model have the opposite probabilities
		Can think of it as both the background and topic model collaborating and competing for high probability for words
		Generally, if p(w1|theta_B) > p(w2|theta_B) then p(w1|theta_d) < p(w2|theta_d)
		
	Probabilistic Topic Models: Mixture Model Estimation: Part 2:
		If we add more occurrences of the same word, the probability of generating that word with the topic model would also increase
		If we increase the probability of choosing the background model p(theta_B), the importance of the topic model decreases so it becomes less important for theta_d to give high probabilities to words that the background model does
		You can regularize the high frequency words getting higher probability with the topic model by changing the probability of picking the background model
		
	Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1:
		If we know which word is from which distribution, we can figure out the word probabilities in the topic model easily by normalizing their counts in the psuedo document (which is all of the words generated from the topic model in the original document)
		If we knew all the parameters for the mixture model, we can infer which distribution each word came from. Our prior is the probability of picking each model p(theta_d) and p(theta_B) and assuming they are equal, our initial guess for where a word came from will be which topic model gives it the highest probability
		Introduce a new variable Z which is 0 if the word is from the topic model and 1 if it is from the background model. Then p(z=0|w) = p(theta_d)p(w|theta_d)/[p(theta_d)p(w|theta_d)+p(theta_B)p(w|theta_B)] which is just the normalized probability that z=0
		
	Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2:
		The Z variable introduced before is treated as a hidden variable
		The expectation-maximization algorithm (EM) is:
			Initialize p(w|theta_d) with random values
			Iteratively improve it using E-step and M-step 
			Stop when likelihood doesn't change
		The E-step is calculating p(z=0|w) or the probability that a word is from the topic model and stores that value
		The M-step calculates the p(w|theta_d) or the probability that a word is generated from the topic model given the probability that the word is from the topic model and this value is also stored so we can iterate these steps
		
	Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3:
		EM algorithm is a hill climbing algorithm so it will always converge to a local maximum, not neccisarily a local maximum depending on the initial guess
		From your current guess, you improve it by maximizing the lower bound and then map it back to the original likelihood function. The E-step is computing the lower bound and the M-step is maximizing the lower bound
		The E-step augments the data by predicting values of hidden variables and the m-step exploits the augmented data to improve parameter estimation
		
	Probabilistic Latent Semantic Analysis (PLSA): Part 1:
		Can be used to mine multiple topics in a document
		In this case, the input is the collection C, number of topics k, and the vocabulary V. The output is a set of word distributions (theta_i,...,theta_k) and a topic coverage matrix (pi_ij)
		PLSA is like the two topic mixture model discussed above, just with more topics added
		The probability of picking the background model is a parameter called lambda_B, so the probability of choosing an actual topic is 1-lambda_B
		Then the probability of choosing a topic is given by pi_d1 to pi_dk
		So the probability of choosing a word with this model is: lambda_B * p(w|theta_B) + [(1-lambda_B) * p(theta_k) * p(w|theta_k) for all k topics]
		The unknown parameters are the topic coverages pi_ij and the word distributions theta_j
		Say p(C|Lambda) is the probability of generating a collection given all of the parameters, then the ML parameter estimation is argmax_Lambda p(C|Lambda)
		
	Probabilistic Latent Semantic Analysis (PLSA): Part 2:
		We can compute the ML estimator using the EM algorithm
		Now our hidden variable Z can take on k+1 values where B (or 0) denotes the background model and all of the other values denote topic k 
		So our E-step calculates the probability that the word is from topic j and we also calculate the probability that the word is from the background model
		We also need to use the document to index the word since now we use pi_dj in the calculation where d is the document
		Now the M-step reestimates the topic coverage probabilities as well as the word distributions
		Note that when normalizing now, we normaize over all topics for the topic coverage, and normalize over all documents in the collection for the distributions
		We still randomly initialize all the unknown parameters and then repeat the E and M steps until likelihood converges
		We can also normalize at the end of each step to simplify the calculation. Accumulate the counts and then normalize
		
	Latent Dirichlet Allocation (LDA): Part 1:
		PLSA with prior knowledge is user-controlled PLSA, PLSA as a generative model is LDA
		PLSA with prior knowledge is useful when users have expectations about which topics to analyze and what topics are (or are not) covered in a document
		We can use tags as topics, so the a doc can only be generated using topics corresponding to the tags assigned to it
		In this case, we can use the MAP estimate, where the ML estimate is argmax_Lambda p(Lambda)p(Data|Lambda). We can use p(Lambda) to encode all of the preferences and constraints 
		The MAP estimate (with conjugate prior) can be computed using a similar EM algorithm to the ML estimate with smoothing to reflect prior preferences
		Say our prior was that we want a topic distribution to be very similar to a given one. Then the M-step would get modified by adding psuedo counts of words from the prior distribution to give more values to those words. This also means adding a new variable mu which when set to 0, means ignoring the prior, and setting it to infinity means to only use the prior
		We can also set any parameter to a constant as needed (when there is prior knowledge that suggests this)
		
	Latent Dirichlet Allocation (LDA): Part 2:
		PLSA is not a generative model because you need pi and it has many parameters which makes it complex and prone to overfitting
		LDA is imposing a Dirichlet prior on the PLSA model parameters (LDA is the Bayesian version of PLSA)
		LDA imposes a prior on both the thetas and pis by setting p(pi_d)=Dirichlet(alpha) and p(theta_i)=Dirichlet(beta) where alpha and beta are more parameters. Dirichlet tells us information on which values for pis and alphas are more likely
		So we draw values for pi from the Dirichlet distribution, and then use them to determine which topic we are choosing. Similarly, we draw the word distributions from the Dirichlet distribution
		In both PLSA and LDA, the probability of generating a word, given the word distribution for a topic and the probability of choosing that topic, are the same
		For LDA, you add integrals to the probability of generating documents and collections to cover the uncertainties of the values of theta and pi
		The ML estimator is argmax_alpha,beta log p(C|alpha, beta) so there are fewer parameters now
		But now thetas and pis must be computed using posterior inference given alpha and beta so we must resort to approximate inferences. They are computed based on Bayesian inference 
		PLSA is the basic topic model and is often adequate
		
		
		
Week 10:
	Text Clustering: Motivation:
		Text clustering is discovering natural structure in data and grouping similar objects together
		A user must define the perspective (bias) for assessing similarity
		
	Text Clustering: Similarity-based Approaches:
		Explicitly define a similarity function to measure similarity between two text objects
		Find an optimal partitioning of data to maximize intra-group similarity and minimize inter-group similarity
		Two strategies for obtaining optimal clustering:
			Progressively construct a hierarchy of clusters (either top-down or bottom-up)
			Start with an initial tentative clustering and iteratively improve it
		In agglomerative hierarchical clustering, you are given a similarity function and gradually group similar objects together in a bottom up fashion to form a hierarchy and stop when a stopping criterion is met
		Start by finding pairs of most similar text objects and keep grouping them together to get a binary tree
		Three ways to compute group similarity:
			Single-link which is similarity of the closest pair, generates loose clusters, is individual decision and is sensitive to outliers
			Complete-link is the similarity of the furthest pair, generates tight clusters, is individual decision and is sensitive to outliers
			Average-link is the average similarity of all pairs of the two groups, generates in between clusters, is group decision and is insensitive to outliers
		The best one depends on the use case
		In k-means clustering, represent each text object as a term vector and assume a similarity function defined on two objects
		Start with k randomly selected vectors and assume they are the centroids of k clusters, then assign every vector a cluster whose centroied is closest to the vector and re-compute the centroid for each cluster based on the newly assigned vectors in the cluster and repeat until the similarity function converges
		Similarity based approchaes allows for direct and flexible specification of similarity but the objective function to be optimized is not always clear (like in k-means)
		Both similarity based and model based approaches can generate both term and doc clusters
		
	Text Clustering: Evaluation:
		The perspective becomes important for evaluation too
		We want to figure out how close the system generated clusters are to ideal human generated clusters
		The evaluation procedure is:
			Have humans create an ideal clustering result of a test set
			Have a system produce clusters using the test set
			Quantify the similarity between the system and human generated clusters
		Indirect evaluation is answering how useful the clustering results are for the inteded applications
		The procedure for this is:
			Create a test set for the intended application to quantify the performance of any system for this application
			Choose a baseline system to compare with
			Add a clustering algorithm to the baseline system
			Compare the performance of the clustering system and the baseline in terms of any performance measure for the application
		Text clustering is an unsupervised general text mining technique
		Strong clusters tend to show up no matter what method is used
		Effectiveness of a method depends on whether the desired clustering bias is captured appropriately
		Deciding the optimal number of clusters is generally a difficult problem
	
	Text Categorization: Motivation:
		Given a set of predefined categories and a training set, the goal is to classify a text object into one or more of the categories
		Internal catgeories characterize a text object
		External categories characterize an entity associated with the text object such as the author
		Binary categorization is when there are only two categories, k-category is when there are more than two
		Hierarchical categorization is when the categories form a hierarchy
		Joint categorization is when multiple related categorization tasks are done in a join matter
		Text categorization helps enrich text representation and can be used to infer properties of entities associated with text data
	
	Text Categorization: Methods:
		Manual categorization works well when the categories are well defined, easily distinguished based on surface features in text and sufficient domain knowledge is available to suggest many effective rules
		Problems with manual categorization is that it is labor intensive (so it doesn't scale well) and can't handle uncertainty in rules so the rules may be inconsistent and not robust
		Both of these problems can be solved with machine learning, which is called "automatic" categorization
		In this case, human experts annotate data sets with category labels (training data) and provide a set of features to represent each text object to provide clues for which category they belong to
		Then you use machine learning to learn "soft rules" from the training data which involves:
			Figuring out which features are most useful for seperating different categories
			Optimally combining the features to minimize the errors of categorization on the training data
		The trained classifier can then be applied to new text objects to predict the most likely categories
		The general setup is to learn a classifier function f that maps text objects to categories
		All methods rely on discriminative features of text objects, combine multle features in a weighted manner and adjust weights on features to minimize error on the training data
		Different methods tend to vary in their way of measuring the error on the training data and combining features
		Generative classifiers learn what the data looks like in each category such as Naive Bayes. The objective function is likelihood, thus indirectly measuring training errors
		Discriminative classifiers learn what features seperate categories, Objective function directly measures errors of categorization on training data. Some examples are logistic regression, kNN, SVM
	
	Text Categorization: Generative Probabilistic Models:
		This can be thought of as similar to document clustering but with known categories/topics
		The problem can be rephrased as which theta_i was used to generate a document/word
		We can define a function cluster(d) = argmax_i p(theta_i | d) = argmax_i p(d|theta_i)p(theta_i). We use the prior p(theta_i) to calculate this posterior probability with the MAP probability
		Take the logarithm of the category(d) function to avoid underflow from multiplying a lot of small probabilities together
		We can make theta_i represent category i accurately using the training data
		When working with the training data, we have a generative model and not a mixture model because we already know which category the document is from. The goal is to estimate the prior p(theta_i) and the word distributions p(w|theta_i)
		To estimate the prior p(theta_i) the idea is that if you have seen more documents for category i, p(theta_i) would be higher (proportional to |T_i|) and the same idea for the word distribution (proportional to c(w,T_i))
		We need to use smoothing to address data sparseness (words with 0 occurrences would have 0 probability), incorporate prior knowledge and achieve discriminative weighting
		Smoothing for the prior is done by adding pseudo counts (delta) so that every category gets at least a delta count. delta is >= 0 and if it were set to infinity, then every category has an infinite amount of documents so there is not distinguishing them
		Smoothing for the word distribution is done by adding a non-uniform pseudocount (mu). So instead we add mu*p(w|theta_B) and introduce a background model. This will give more common words higher pseudo counts which will bring probability of common words higher resulting in probability of those words smaller across other categories and vice versa. mu still controls the amount of smoothing and if it was set to infinity, the word distribution will become more similar to the background model. Also, if we instead used uniform smoothing instead of the background model word distribution, it becomes similar to the smoothing for the prior
		For two categories theta_1 and theta_2, we can score a document score(d)=log(p(theta_1|d)/p(theta_2|d)). So the higher score(d) is, the more likely it is in category 1
		We can denote the category bias which does not depend on d as beta_0, the features which are the counts of the words in the document as f_i, and the weight on each feature as beta_i. Then we can generalize the score by denoting a document as a feature vector and rewriting the score function as score(d) = beta_0 + the sum of f_i*beta_i for all words. This way we can add more to the features such as document length and author. This general form is very close to logistic regression which is actually discriminative
		
		
		
Week 11:
	Text Categorization: Discriminative Classifier Part 1:
		In logistic regression, we have a binary response variable Y and we provide as input a vector X which are our predictors
		This allows many features other than just words. The format of this looks very similar to the scoring function for Naive Bayes
		Getting rid of the logs gives us that p(Y=1)|X)=a function that is between 0 and 1 for all X
		We want to estimate the value of the parameters which are the elements of the beta vector
		If our training data is denoted as T, the conditional likelihood is p(T|beta)=product of p(Y=Y_i|X=X_i, beta) for all training points i
		Then the ML estimate is beta* = argmax_beta p(T|beta) which can be computed in many ways
		In k-nearest neighbors (k-NN), you find k examples in the training set that are most similar to the text object to be classified and assign the object the category that is most common between these neighbors
		kNN can be improved by considering the distance of a neighbor and can be regarded as a way to directly estimate the conditional probability of label given a data instance (p(Y|X)). It also requires an explicit similarity function
		One of the challenges with kNN is figuring out what value to use for k which can often be figured out with cross validation
		kNN makes the assumption that p(theta_i|d) is locally smooth or that it is the same for all docs in the region R created by all of the neighbors
		We can estimate p(theta_i, R) as c(theta_i, R)/|R| or the number of docs in R with category i divided by the total number of docs in R 
	
	Text Categorization: Evaluation Part 1:
		General evaluation methodology:
			Have humans create a test collection where documents are tagged with their categories
			Generate categorization results using a system on the test collection
			Compare the system categorization decisions with the human made decisions and quantify their similarity/difference
		One measure is classification accuracy where we count the number of docs that were/were not assigned a category by the system that humans also did/did not do the same for. So we add up the number of correct decisions made by the system and divide by the total number of decisions made
		This treats all decisions equally which may not always be the case and if you have an imbalanced test set, then you can get a high score even if you don't classify anything correctly for a specific topic
		Per-document evaluation is when we look at how good decisions are made on specific documents
		Precision is measuring when the system says yes, how many are correct (true positives / true positives + false positives)
		Recall is measuring if the document has all the categories it should have (true positives / true positives + false negatives)
		We can also look at per-category evaluation and look at the precision and recall for each category
		Precision and recall can be combined with the f-measure. This is the harmonic mean of precision and recall. F_1 can be measured as 2PR/P+R and can be controlled with a parameter beta
	
	Text Categorization: Evaluation Part 2:
		We can compute precision, recall, and f-measure for all categories and aggregate them to compute an overall macro average. There are different ways to aggregate such as arithmetic mean or geometric mean. The same can be done for all documents isntead of categories
		Micro averaging is to pool together all of the decisions and then compute the precision/recall instead of it doing for each category/document first. This is similar to classification accuracy so it is generally less informative than macro averages
		Sometimes ranking is more appropriate such as when the results are passed to a human for further editing or prioritizing tasks. We can evaluate the results as a ranked list if the system can give scores for the decisions
		
	Opinion Mining and Sentiment Analysis: Motivation:
		An opinion can be formally defined as a subjective statement describing what a person believes or thinks about something
		An opinion representation should specify the opinion holder, target, and content. To enrich the representation, you can also have the opinion context and sentiment
		There can be an authors opinion (the reviewers opinion) or a reported opinion (the reviewers opinion about what someone else thinks) or an inferred opinion
		The task of opinion of mining is taking text data and turning it into a set of opinion representations
		Opinion mining is useful for decision support, understanding people, and voluntary surveys (using humans as sensors and aggregating opinions)
		
	Opinion Mining and Sentiment Analysis: Sentiment Classification:
		Suppose we know everything in the opinion representation except for the sentiment
		The input is an opinionated text object and the output is a sentiment tag/label which can either be a polarity (positive, neutral, negative), or an emotion. This can be thought of as a special case of text categorization
		Some commonly used text features for sentiment analysis are character n-grams (robust to spelling/recognition errors but not as discriminative as words), word n-grams (unigrams not as good for sentiment and long n-grams may cause overfitting), POS tag n-grams, word classes, frequent patterns in text (may generalize better than pure n-grams), parse tree-based. Pattern discovery algorithms are very useful for feature construction
		NLP enriches text representation with complex features (POS tags, parse trees, entities, speech acts) and they can be mixed such as "great NOUN"
		Feature design affects categorization accuracy by a lot. A combination of machine learning, error analysis, and domain knowledge is most effective
		Need to watch out for overfitting when the feature space gets enriched
		Want to optimize the tradeoff between exhaustivity and speficifity which can be difficult
		
		
		
Week 12:
	Text-Based Prediction:
		In the "data mining loop" humans are involved in the joint mining of non-text and text data, building the predictive model, taking actions to change the world, and controlling the sensor to collect most useful data
		Some questions to answer are: how can we generate effective predictors from text, how can we jointly mine text and non-text data
		Texxt based prediction can be seens a unified framework of text mining and analysis. The main task is to infer values of real-world variables with subtasks being mining content of text data and mining knowledge about the observer
		Non-text data can help text mining by providing context for the text mining
		Text data can help non-text data mining because it can help interpret patterns discovered from non-text data
		
	Contextual Text Mining: Motivation:
		Text often has rich context information (direct and indirect context) and any related data can be regarded as data
		Context can be used to partition text data and provide meaning to the discovered topics
		
	Contextual Text Mining: Contextual Probabilistic Latent Semantic Analysis:
		CPLSA is an extension of the PLSA model. The idea is to explicitly add interesting context variables into a generative model so that the context influences both coverage and content variation of topics
		We model the conditional likelihood of text given context, and assume context dependent views of a topic and topic coverage
		With context, we can create views which are basically perspectives of the themes with a condition. For example, your view could be anything from July 2005. This will give you a specific version of word distributions based on the chosen view which will the problem look similar to normal PLSA
		We also choose a theme coverage which is tied to the view that you choose. This is a fixed coverage at the start and each document has it's own coverage. The context can change the coverage. Since the context influences our choices (there are extra switches) the model will have more parameters
		An example use case would be to compare news articles for two different wars. If one of the themes is a common theme and the other two are the two wars, you can see the common theme's influence on the two wars in each cluster
		
	Contextual Text Mining: Mining Topics with Social Media Context:
		The context of a text article can form a network such as authors through social networks and locations. We can gain some benefit of joint analysis of the text and its network context. Networks impose constraints on topics in text (connected authors tend to write about similar topics) and text can characterize the content associated with a subnetwork
		Network supervised topic modeling is probabilistic modeling as optimization where the objective function is maximum likelihood
		The main idea is that the network imposes constraints on the model parameters Lambda. The objective function then is armgax_ambda f(p(TextData|Lambda),r(Lambda,Network)). Topic distributions are smoothed over adjacent nodes and we add network-induced regularizers r to the likelihood objective function
		The topic model can be any generative model, the network can be any kind of network, r can be any regularizer, and f can combine the two in any way
		The network induced prior is that neighbors have similar topic distribution. The idea is to quantify the difference in topic coverage over adjacent nodes and subtract it from the PLSA log likelihood. Since we want to maximize the objective function, we want to minimize the difference in topic coverages
		We can also take edge weight into consideration and we can introduce another lambda variable to control the influence of the network constraint
		
	Contextual Text Mining: Mining Causal Topics with Time Series Supervision:
		We want to use text mining to understand a time series. So if we have a time series and a news stream for example, we want to explain changes in the graph
		The input here is a time series and the text data produced in the same time period. The output is the topics whose coverage in the text stream has strong correlations with the time series ("causal topics")
		The strategies we discussed so far are not the best because they pick topics that maximize the likelihood for the text data, and not for their correlation to the time series
		Iterative causal topic modeling is when you take your text stream and apply regular topic modeling to it. Then you take your time series and use it to assess which topics are more correlated to the time series. Then we zoom into the word level for the best topics and look for causal words (highly correlated words with the time series). Then we split the words that were positively and negatively correlated into their own subtopics and feed it back into the topic model as a prior
		Doing this will help maximize both the topic time series causality and topic coherence
		A commonly used measure for causality is the Granger Causality Test which uses the history of the time series to try to predict new values in the time series and then adds the history of the topic coverage to see if it can be improved. If it can be improved, we can say that there is causality
		Sometimes we want to add a lag so that there is time for the text data to reflect changes in the time series
		Text based prediction is useful for big data applications

		
		
		
		
		
		
		
	
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		